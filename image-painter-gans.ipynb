{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Painter GANs\n",
    "Implemented by Kangze Huang\n",
    "\n",
    "Based off the paper \"Globally and Locally Consistent Image Completion\" by Satoshi Iizuka et al [Waseda University, Japan 2017]\n",
    "http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4aa8948493b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubtract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Reshape, Lambda, Flatten, Activation, Conv2D, Conv2DTranspose, Dense, Input, Subtract, Add, Multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine.network import Network\n",
    "from keras.optimizers import Adadelta\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_shape = (256,256,3)\n",
    "local_shape = (128,128,3)\n",
    "optimizer = Adadelta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the GANs\n",
    "From scratch, and combining each neural net together, until we create a master brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primative Generator Net\n",
    "This does not include the masks, we only define the images being inputted. We will add the masks later (turning into an augmented net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_generator(input_shape=(256, 256, 3)):\n",
    "    in_layer = Input(shape=input_shape)\n",
    "\n",
    "    model = Conv2D(64, kernel_size=5, strides=1, padding='same',\n",
    "                     dilation_rate=(1, 1))(in_layer)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(128, kernel_size=3, strides=2,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(128, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=2,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(2, 2))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(4, 4))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(8, 8))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(16, 16))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(128, kernel_size=4, strides=2,\n",
    "                              padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(128, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(64, kernel_size=4, strides=2,\n",
    "                              padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(32, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(3, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('sigmoid')(model)\n",
    "    model_gen = Model(inputs=in_layer, outputs=model)\n",
    "    model_gen.name = 'Gener8tor'\n",
    "    return model_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primative Discriminator Net\n",
    "This accounts for the masks and input images, but is not connected to anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_discriminator(global_shape=(256, 256, 3), local_shape=(128, 128, 3)):\n",
    "    def crop_image(img, crop):\n",
    "        return tf.image.crop_to_bounding_box(img,\n",
    "                                             crop[1],\n",
    "                                             crop[0],\n",
    "                                             crop[3] - crop[1],\n",
    "                                             crop[2] - crop[0])\n",
    "\n",
    "    in_pts = Input(shape=(4,), dtype='int32')\n",
    "    cropping = Lambda(lambda x: K.map_fn(lambda y: crop_image(y[0], y[1]), elems=x, dtype=tf.float32),\n",
    "                      output_shape=local_shape)\n",
    "    g_img = Input(shape=global_shape)\n",
    "    l_img = cropping([g_img, in_pts])\n",
    "\n",
    "    # Local Discriminator\n",
    "    x_l = Conv2D(64, kernel_size=5, strides=2, padding='same')(l_img)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(128, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(256, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Flatten()(x_l)\n",
    "    x_l = Dense(1024, activation='relu')(x_l)\n",
    "\n",
    "    # Global Discriminator\n",
    "    x_g = Conv2D(64, kernel_size=5, strides=2, padding='same')(g_img)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(128, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(256, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Flatten()(x_g)\n",
    "    x_g = Dense(1024, activation='relu')(x_g)\n",
    "\n",
    "    x = Concatenate(axis=1)([x_l, x_g])\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model_disc = Model(inputs=[g_img, in_pts], outputs=x)\n",
    "    model_disc.name = 'Discimi-hater'\n",
    "    return model_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_models(model, filename):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file=filename, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Generator Net\n",
    "We connect the masks now, turning the primitive net more advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_gen_layer(full_img, mask, ones):\n",
    "    from keras.layers import Concatenate\n",
    "\n",
    "    # grab the inverse mask, that only shows the masked areas\n",
    "    # 1 - mask\n",
    "    inverse_mask = Subtract()([ones, mask])\n",
    "\n",
    "    # which outputs the erased_image as input\n",
    "    # full_img * (1 - mask)\n",
    "    erased_image = Multiply()([full_img, inverse_mask])\n",
    "\n",
    "    # view our net\n",
    "    gen_model = model_generator(global_shape)\n",
    "    # print(gen_model)\n",
    "\n",
    "    # pass in the erased_image as input\n",
    "    gen_model = gen_model(erased_image)\n",
    "    # print(gen_model)\n",
    "\n",
    "    gen_brain = Model(inputs=[full_img, mask, ones], outputs=gen_model)\n",
    "    # print(gen_brain)\n",
    "    view_models(gen_brain, 'summaries/gen_brain.png')\n",
    "\n",
    "    gen_brain.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    # gen_brain.summary()\n",
    "    return gen_brain, gen_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Discriminator Net\n",
    "We connect the primitive discriminator net to the output of the augmented generator net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_disc_layer(global_shape, local_shape, full_img, clip_coords):\n",
    "    # the discriminator side\n",
    "    disc_model = model_discriminator(global_shape, local_shape)\n",
    "\n",
    "    disc_model = disc_model([full_img, clip_coords])\n",
    "    disc_model\n",
    "    # print(disc_model)\n",
    "\n",
    "    disc_brain = Model(inputs=[full_img, clip_coords], outputs=disc_model)\n",
    "    disc_brain.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer)\n",
    "    # disc_brain.summary()\n",
    "    view_models(disc_brain, 'summaries/disc_brain.png')\n",
    "    return disc_brain, disc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65867558da68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfull_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclip_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "full_img = Input(shape=global_shape)\n",
    "clip_img = Input(shape=local_shape)\n",
    "mask = Input(shape=(global_shape[0], global_shape[1], 1))\n",
    "ones = Input(shape=(global_shape[0], global_shape[1], 1))\n",
    "clip_coords = Input(shape=(4,), dtype='int32')\n",
    "\n",
    "gen_brain, gen_model = full_gen_layer(full_img, mask, ones)\n",
    "disc_brain, disc_model = full_disc_layer(global_shape, local_shape, full_img, clip_coords)\n",
    "\n",
    "print(gen_brain)\n",
    "print(disc_brain)\n",
    "\n",
    "print(gen_model)\n",
    "print(disc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-18feddb43b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# the final brain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconnected_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconnected_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Connected-Discrimi-Hater'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'disc_model' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.0004\n",
    "\n",
    "# the final brain\n",
    "disc_model.trainable = False\n",
    "connected_disc = Model(inputs=[full_img, clip_coords], outputs=disc_model)\n",
    "connected_disc.name = 'Connected-Discrimi-Hater'\n",
    "print(connected_disc)\n",
    "\n",
    "brain = Model(inputs=[full_img, mask, ones, clip_coords], outputs=[gen_model, connected_disc([gen_model, clip_coords])])\n",
    "brain.compile(loss=['mse', 'binary_crossentropy'],\n",
    "                      loss_weights=[1.0, alpha], optimizer=optimizer)\n",
    "brain.summary()\n",
    "view_models(brain, 'summaries/brain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Image Preprocessor\n",
    "Using a memory-efficient Python generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ffeeaed0fb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kangzehuang/.local/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kangzehuang/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kangzehuang/.local/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "from google.cloud import storage\n",
    "import google\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# creds, _ = google.auth.default()\n",
    "client = storage.Client()\n",
    "bucket = client.bucket('lsun-roomsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dbb0afec8138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# playable version of DataGenerator()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFuckAround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# list the bucket and directory within\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbucketname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://lsun-roomsets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'images/bedroom_val/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dbb0afec8138>\u001b[0m in \u001b[0;36mFuckAround\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# store the raw img urls here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimg_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket' is not defined"
     ]
    }
   ],
   "source": [
    "# playable version of DataGenerator()\n",
    "class FuckAround():\n",
    "    # list the bucket and directory within\n",
    "    bucketname = 'gs://lsun-roomsets'\n",
    "    directory = 'images/bedroom_val/'\n",
    "    \n",
    "    # loop through all the files and view first X images (count)\n",
    "    count = 0\n",
    "    max_count = 10\n",
    "    # store the raw img urls here\n",
    "    img_urls = []\n",
    "    for blob in bucket.list_blobs(prefix=directory):\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(blob.name)\n",
    "        count += 1\n",
    "        img_urls.append(blob.name)\n",
    "        \n",
    "    # store the resized images here\n",
    "    images = []\n",
    "    points = []\n",
    "    masks = []\n",
    "    \n",
    "    # CONSTANTS\n",
    "    # mask size limits\n",
    "    hole_min = 64\n",
    "    hole_max = 128\n",
    "    # batch limits\n",
    "    batch_size = 5\n",
    "    max_batches = 3\n",
    "    batch_count = 0\n",
    "    # image sizes\n",
    "    image_size = (256,256)\n",
    "    local_size = (128,128)\n",
    "    \n",
    "    for idx, img_url in enumerate(img_urls):\n",
    "        # we use tf...file_io.FileIO to grab the file\n",
    "        with file_io.FileIO(f'{bucketname}/{img_url}', 'rb') as f:\n",
    "            # and use PIL to convert into an RGB image\n",
    "            img = Image.open(f).convert('RGB')\n",
    "            # then convert the RGB image to an array so that cv2 can read it\n",
    "            img = np.asarray(img, dtype=\"uint8\")\n",
    "            # resize images\n",
    "            img_resized = cv2.resize(img, image_size)[:,:,::-1]\n",
    "            # take a look at the images\n",
    "            # cv2.imshow(f'image_{idx}_resized', img_resized)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyWindow(f'image_{idx}_resized')\n",
    "            # add the resized photo to self.images\n",
    "            images.append(img_resized)\n",
    "            print(f'{idx}. Processing {img_url}')\n",
    "            \n",
    "            # now lets create the random points where we will apply a mask (erase parts of image)\n",
    "            # recall that image_size=(256,256) and local_size=(128,128)\n",
    "            x1 = np.random.randint(0, image_size[0] - local_size[0] + 1)\n",
    "            y1 = np.random.randint(0, image_size[1] - local_size[1] + 1)\n",
    "            x2, y2 = np.array([x1, y1]) + np.array(local_size)\n",
    "            points.append([x1,y1,x2,y2])\n",
    "            \n",
    "            # and we also randomly generate width and height of those masks\n",
    "            w, h = np.random.randint(hole_min, hole_max, 2)\n",
    "            p1 = x1 + np.random.randint(0, local_size[0] - w)\n",
    "            q1 = y1 + np.random.randint(0, local_size[1] - h)\n",
    "            p2 = p1 + w\n",
    "            q2 = q1 + h\n",
    "            # now create the array of zeros\n",
    "            m = np.zeros((image_size[0], image_size[1], 1), dtype=np.uint8)\n",
    "            # everywhere there should be the mask, make the value one (everywhere else is zero)\n",
    "            m[q1:q2 + 1, p1:p2 + 1] = 1\n",
    "            # finally append it to the self.masks\n",
    "            masks.append(m)\n",
    "            \n",
    "            # print the batch of data when batch size reached\n",
    "            if len(images) == batch_size:\n",
    "#                 print(np.array(images).shape)\n",
    "#                 print(np.array(points).shape)\n",
    "#                 print(np.array(masks).shape)\n",
    "                inputs = np.asarray(images, dtype=np.float32) / 255\n",
    "                points = np.asarray(points, dtype=np.int32)\n",
    "                masks = np.asarray(masks, dtype=np.float32)\n",
    "                \n",
    "                # reset\n",
    "                images = []\n",
    "                points = []\n",
    "                masks = []\n",
    "                batch_count += 1\n",
    "                \n",
    "            if batch_count > max_batches:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerator\n",
    "Using a memory-efficient Python generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    # initialize by retreiving the photos\n",
    "    def __init__(self, bucketname, input_dir, image_size, local_size):\n",
    "        # bucketname = 'gs://lsun-roomsets'\n",
    "        # input_dir = 'images/bedroom_train/'\n",
    "        # image_size = (256,256)\n",
    "        # local_size = (128,128)\n",
    "        self.image_size = image_size\n",
    "        self.local_size = local_size\n",
    "        self.reset()\n",
    "        self.img_file_list = []\n",
    "        # for now we get max self.count photos and add them to self.img_file_list\n",
    "        for blob in bucket.list_blobs(prefix=input_dir):\n",
    "            self.img_file_list.append(blob.name)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_file_list)\n",
    "    \n",
    "    # we also track the preprocessed images, points, and masks\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "        self.points = []\n",
    "        self.masks = []\n",
    "    \n",
    "    # iterates over self.img_file_list and does preprocessing\n",
    "    def flow(self, batch_size, hole_min=64, hole_max=128):\n",
    "        np.random.shuffle(self.img_file_list)\n",
    "        for idx, img_url in enumerate(self.img_file_list):\n",
    "            # we use tf...file_io.FileIO to grab the file\n",
    "            with file_io.FileIO(f'{bucketname}/{img_url}', 'rb') as f:\n",
    "                # and use PIL to convert into an RGB image\n",
    "                img = Image.open(f).convert('RGB')\n",
    "                # then convert the RGB image to an array so that cv2 can read it\n",
    "                img = np.asarray(img, dtype=\"uint8\")\n",
    "                # resize images\n",
    "                img_resized = cv2.resize(img, self.image_size)[:,:,::-1]\n",
    "                # take a look at the images\n",
    "                # cv2.imshow(f'image_{idx}_resized', img_resized)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyWindow(f'image_{idx}_resized')\n",
    "                # add the resized photo to self.images\n",
    "                self.images.append(img_resized)\n",
    "\n",
    "                # now lets create the random location (aka. X,Y points) where we will apply a mask (aka. erase parts of image)\n",
    "                # recall that image_size=(256,256) and local_size=(128,128)\n",
    "                x1 = np.random.randint(0, self.image_size[0] - self.local_size[0] + 1)\n",
    "                y1 = np.random.randint(0, self.image_size[1] - self.local_size[1] + 1)\n",
    "                x2, y2 = np.array([x1, y1]) + np.array(self.local_size)\n",
    "                self.points.append([x1,y1,x2,y2])\n",
    "                # and we also randomly generate width and height of those masks\n",
    "                w, h = np.random.randint(hole_min, hole_max, 2)\n",
    "                p1 = x1 + np.random.randint(0, self.local_size[0] - w)\n",
    "                q1 = y1 + np.random.randint(0, self.local_size[1] - h)\n",
    "                p2 = p1 + w\n",
    "                q2 = q1 + h\n",
    "                # now create the array of zeros\n",
    "                m = np.zeros((self.image_size[0], self.image_size[1], 1), dtype=np.uint8)\n",
    "                # everywhere there should be the mask, make the value one (everywhere else is zero)\n",
    "                m[q1:q2 + 1, p1:p2 + 1] = 1\n",
    "                # finally append it to the self.masks\n",
    "                self.masks.append(m)\n",
    "\n",
    "                # yeild the batch of data when batch size reached\n",
    "                if len(self.images) == batch_size:\n",
    "                    images = np.asarray(self.images, dtype=np.float32) / 255\n",
    "                    points = np.asarray(self.points, dtype=np.int32)\n",
    "                    masks = np.asarray(self.masks, dtype=np.float32)\n",
    "                    self.reset()\n",
    "                    yield images, points, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Start the Training\n",
    "With hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a9c176c72c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.utils import generic_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-640b18db4ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-cd6b38120f33>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bucketname, input_dir, image_size, local_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_file_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# for now we get max self.count photos and add them to self.img_file_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_file_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket' is not defined"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "input_shape = (256, 256, 3)\n",
    "local_shape = (128, 128, 3)\n",
    "batch_size = 2\n",
    "epochs = 1\n",
    "g_epochs = int(epochs * 0.18) # should be 90k on generator\n",
    "d_epochs = int(epochs * 0.02) # should be 10k on discriminator\n",
    "alpha = 0.0004\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "# input/output directories\n",
    "bucketname = \"gs://lsun-roomsets\"\n",
    "output_dir = \"outputs/\"\n",
    "input_dir = \"images/bedroom_val/\"\n",
    "\n",
    "# data generator\n",
    "train_datagen = DataGenerator(bucketname, input_dir, input_shape[:2], local_shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9b641c05d964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# progress bar visualization (comment out in ML Engine)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datagen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# and the matrix of ones that we depend on in the neural net to inverse masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generic_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# train over time\n",
    "for epoch in range(epochs):\n",
    "    # progress bar visualization (comment out in ML Engine)\n",
    "    progbar = generic_utils.Progbar(len(train_datagen))\n",
    "    for images, points, masks in train_datagen.flow(batch_size):\n",
    "        # and the matrix of ones that we depend on in the neural net to inverse masks\n",
    "        mask_inv = np.ones((len(images), input_shape[0], input_shape[1], 1))\n",
    "        # generate the inputs (images)\n",
    "        generated_img = gen_brain.predict([images, masks, mask_inv])\n",
    "        # generate the labels\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        # the gen and disc losses\n",
    "        g_loss = 0.0\n",
    "        d_loss = 0.0\n",
    "        \n",
    "        # we must train the neural nets seperately, and then together\n",
    "        # train generator for 90k epochs\n",
    "        if epoch < g_epochs:\n",
    "            # set the gen loss\n",
    "            g_loss = gen_brain.train_on_batch([images, points], valid)\n",
    "        # train discriminator alone for 90k epochs\n",
    "        # then train disc + gen for another 400k epochs. Total of 500k\n",
    "        else:\n",
    "            # throw in real unedited images with label VALID\n",
    "            d_loss_real = disc_brain.train_on_batch([images, points], valid)\n",
    "            # throw in A.I. generated images with label FAKE\n",
    "            d_loss_fake = disc_brain.train_on_batch([generated_img, points], fake)\n",
    "            # combine and set the disc loss\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            if epoch >= g_epochs + d_epochs:\n",
    "                # train the entire brain\n",
    "                g_loss = brain.train_on_batch([images, masks, mask_inv, points], [images, valid])\n",
    "                # and update the generator loss\n",
    "                g_loss = g_loss[0] + alpha * g_loss[1]\n",
    "        # progress bar visualization (comment out in ML Engine)\n",
    "        progbar.add(images.shape[0], values=[(\"Disc Loss: \", d_loss), (\"Gen mse: \", g_loss)])\n",
    "        batch_count += 1\n",
    "        # save the generated image\n",
    "        last_img = generated_img[0]\n",
    "        last_img[:,:,0] = last_img[:,:,0]*255\n",
    "        last_img[:,:,1] = last_img[:,:,1]*255\n",
    "        last_img[:,:,2] = last_img[:,:,2]*255\n",
    "        dreamt_image = Image.fromarray(last_img.astype(int), 'RGB')\n",
    "        dreamt_image.save(f\"outputs/images/batch_{batch_count}_image.png\")\n",
    "        \n",
    "    gen_brain.save(f\"outputs/models/batch_{batch_count}_generator.h5\")\n",
    "    disc_brain.save(f\"outputs/models/batch_{batch_count}discriminator.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "Steps left to reach deployment on ML Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Healthy Model\n",
    "1. Verify that `last_img[:,:,0]*255` is actually how to convert 0-1 back to 0-255 RGB (ask a Python guy)\n",
    "2. Verify that our mis-organized gs://bucket/paths dont matter for wildcard `/**/*.png` image retrieval\n",
    "3. Verify that we can actually hold all the img_urls from gs://bucket/train/* in python memory (should be ok since ML engine is serverless)\n",
    "4. Verify that our model is fully convolutional like the paper\n",
    "5. Look for potential pre-trained models online\n",
    "6. Final audit that the model was made right\n",
    "7. Re-factor the iPython Notebook into a Python Module\n",
    "\n",
    "\n",
    "8. Retrieve and display the accuracy/percision metrics during training\n",
    "        - refactor code to use model.fit_to_generator() to get History object with accuracy/percision metrics. find out where we define batch size and epochs etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Building Tools\n",
    "2. Save the last generated image of each epoch to Google Cloud Storage\n",
    "2. Save the accuracy & percision metrics to gs://bucket during training\n",
    "2. Save last best checkpoint of model to gs://bucket. Also able to load model checkpoints to resume training (check if ML engine does that already - unlikely)\n",
    "5. Setup the arguements injection for ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Training\n",
    "1. Setup & Test single-GPU training on ML Engine\n",
    "2. Setup & Test multi-GPU training on ML Engine\n",
    "3. Train fully on ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b0bdae548214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# we use tf...file_io.FileIO to write the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{bucketname}/outputs/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I love Roti'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_io' is not defined"
     ]
    }
   ],
   "source": [
    "# Experimentation\n",
    "\n",
    "# we use tf...file_io.FileIO to write the file\n",
    "with file_io.FileIO(f'{bucketname}/outputs/test.txt', 'wb') as f:\n",
    "    f.write('I love Roti')\n",
    "    \n",
    "with file_io.FileIO(f'{bucketname}/outputs/dreamt.png', 'wb') as f:\n",
    "    dreamt_image.save(f, \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
