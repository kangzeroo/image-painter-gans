{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Painter GANs\n",
    "Implemented by Kangze Huang\n",
    "\n",
    "Based off the paper \"Globally and Locally Consistent Image Completion\" by Satoshi Iizuka et al [Waseda University, Japan 2017]\n",
    "http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Reshape, Lambda, Flatten, Activation, Conv2D, Conv2DTranspose, Dense, Input, Subtract, Add, Multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine.network import Network\n",
    "from keras.optimizers import Adadelta\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_shape = (256,256,3)\n",
    "local_shape = (128,128,3)\n",
    "optimizer = Adadelta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the GANs\n",
    "From scratch, and combining each neural net together, until we create a master brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primative Generator Net\n",
    "This does not include the masks, we only define the images being inputted. We will add the masks later (turning into an augmented net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_generator(input_shape=(256, 256, 3)):\n",
    "    in_layer = Input(shape=input_shape)\n",
    "\n",
    "    model = Conv2D(64, kernel_size=5, strides=1, padding='same',\n",
    "                     dilation_rate=(1, 1))(in_layer)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(128, kernel_size=3, strides=2,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(128, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=2,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(2, 2))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(4, 4))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(8, 8))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(16, 16))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(128, kernel_size=4, strides=2,\n",
    "                              padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(128, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(64, kernel_size=4, strides=2,\n",
    "                              padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(32, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2D(3, kernel_size=3, strides=1,\n",
    "                     padding='same', dilation_rate=(1, 1))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('sigmoid')(model)\n",
    "    model_gen = Model(inputs=in_layer, outputs=model)\n",
    "    model_gen.name = 'Gener8tor'\n",
    "    return model_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primative Discriminator Net\n",
    "This accounts for the masks and input images, but is not connected to anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_discriminator(global_shape=(256, 256, 3), local_shape=(128, 128, 3)):\n",
    "    def crop_image(img, crop):\n",
    "        return tf.image.crop_to_bounding_box(img,\n",
    "                                             crop[1],\n",
    "                                             crop[0],\n",
    "                                             crop[3] - crop[1],\n",
    "                                             crop[2] - crop[0])\n",
    "\n",
    "    in_pts = Input(shape=(4,), dtype='int32')\n",
    "    cropping = Lambda(lambda x: K.map_fn(lambda y: crop_image(y[0], y[1]), elems=x, dtype=tf.float32),\n",
    "                      output_shape=local_shape)\n",
    "    g_img = Input(shape=global_shape)\n",
    "    l_img = cropping([g_img, in_pts])\n",
    "\n",
    "    # Local Discriminator\n",
    "    x_l = Conv2D(64, kernel_size=5, strides=2, padding='same')(l_img)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(128, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(256, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_l)\n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation('relu')(x_l)\n",
    "    x_l = Flatten()(x_l)\n",
    "    x_l = Dense(1024, activation='relu')(x_l)\n",
    "\n",
    "    # Global Discriminator\n",
    "    x_g = Conv2D(64, kernel_size=5, strides=2, padding='same')(g_img)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(128, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(256, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Conv2D(512, kernel_size=5, strides=2, padding='same')(x_g)\n",
    "    x_g = BatchNormalization()(x_g)\n",
    "    x_g = Activation('relu')(x_g)\n",
    "    x_g = Flatten()(x_g)\n",
    "    x_g = Dense(1024, activation='relu')(x_g)\n",
    "\n",
    "    x = Concatenate(axis=1)([x_l, x_g])\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model_disc = Model(inputs=[g_img, in_pts], outputs=x)\n",
    "    model_disc.name = 'Discimi-hater'\n",
    "    return model_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_models(model, filename):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file=filename, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Generator Net\n",
    "We connect the masks now, turning the primitive net more advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_gen_layer(full_img, mask, ones):\n",
    "    from keras.layers import Concatenate\n",
    "\n",
    "    # grab the inverse mask, that only shows the masked areas\n",
    "    # 1 - mask\n",
    "    inverse_mask = Subtract()([ones, mask])\n",
    "\n",
    "    # which outputs the erased_image as input\n",
    "    # full_img * (1 - mask)\n",
    "    erased_image = Multiply()([full_img, inverse_mask])\n",
    "\n",
    "    # view our net\n",
    "    gen_model = model_generator(global_shape)\n",
    "    # print(gen_model)\n",
    "\n",
    "    # pass in the erased_image as input\n",
    "    gen_model = gen_model(erased_image)\n",
    "    # print(gen_model)\n",
    "\n",
    "    gen_brain = Model(inputs=[full_img, mask, ones], outputs=gen_model)\n",
    "    # print(gen_brain)\n",
    "    view_models(gen_brain, 'summaries/gen_brain.png')\n",
    "\n",
    "    gen_brain.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    # gen_brain.summary()\n",
    "    return gen_brain, gen_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Discriminator Net\n",
    "We connect the primitive discriminator net to the output of the augmented generator net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_disc_layer(global_shape, local_shape, full_img, clip_coords):\n",
    "    # the discriminator side\n",
    "    disc_model = model_discriminator(global_shape, local_shape)\n",
    "\n",
    "    disc_model = disc_model([full_img, clip_coords])\n",
    "    disc_model\n",
    "    # print(disc_model)\n",
    "\n",
    "    disc_brain = Model(inputs=[full_img, clip_coords], outputs=disc_model)\n",
    "    disc_brain.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer)\n",
    "    # disc_brain.summary()\n",
    "    view_models(disc_brain, 'summaries/disc_brain.png')\n",
    "    return disc_brain, disc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x11c7aa2e8>\n",
      "<keras.engine.training.Model object at 0x11dfefeb8>\n",
      "Tensor(\"Gener8tor/activation_17/Sigmoid:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "Tensor(\"Discimi-hater/dense_3/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_img = Input(shape=global_shape)\n",
    "clip_img = Input(shape=local_shape)\n",
    "mask = Input(shape=(global_shape[0], global_shape[1], 1))\n",
    "ones = Input(shape=(global_shape[0], global_shape[1], 1))\n",
    "clip_coords = Input(shape=(4,), dtype='int32')\n",
    "\n",
    "gen_brain, gen_model = full_gen_layer(full_img, mask, ones)\n",
    "disc_brain, disc_model = full_disc_layer(global_shape, local_shape, full_img, clip_coords)\n",
    "\n",
    "print(gen_brain)\n",
    "print(disc_brain)\n",
    "\n",
    "print(gen_model)\n",
    "print(disc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x11b9088d0>\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 256, 256, 1)  0           input_4[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 256, 256, 3)  0           input_1[0][0]                    \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Gener8tor (Model)               (None, 256, 256, 3)  6076495     multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Connected-Discrimi-Hater (Model (None, 1)            45070593    Gener8tor[1][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,147,088\n",
      "Trainable params: 51,134,218\n",
      "Non-trainable params: 12,870\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0004\n",
    "\n",
    "# the final brain\n",
    "disc_model.trainable = False\n",
    "connected_disc = Model(inputs=[full_img, clip_coords], outputs=disc_model)\n",
    "connected_disc.name = 'Connected-Discrimi-Hater'\n",
    "print(connected_disc)\n",
    "\n",
    "brain = Model(inputs=[full_img, mask, ones, clip_coords], outputs=[gen_model, connected_disc([gen_model, clip_coords])])\n",
    "brain.compile(loss=['mse', 'binary_crossentropy'],\n",
    "                      loss_weights=[1.0, alpha], optimizer=optimizer)\n",
    "brain.summary()\n",
    "view_models(brain, 'summaries/brain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Image Preprocessor\n",
    "Using a memory-efficient Python generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangzehuang/anaconda3/envs/ComputerVision/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/kangzehuang/anaconda3/envs/ComputerVision/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "from google.cloud import storage\n",
    "import google\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# creds, _ = google.auth.default()\n",
    "client = storage.Client()\n",
    "bucket = client.bucket('lsun-roomsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/bedroom_val/0/0/0/0/0/0/00000089629ce3ba87bae003073896ba01988dee.webp\n",
      "images/bedroom_val/0/0/0/0/0/1/000001ec5684cb40f432f996f8a38e5d076114d8.webp\n",
      "images/bedroom_val/0/0/0/0/0/3/0000036b25b1ae054cdf2e3ee954fe2d21db6ae0.webp\n",
      "images/bedroom_val/0/0/0/0/0/b/00000b94a63ae2e1c08b4e7452d088156a9a8273.webp\n",
      "images/bedroom_val/0/0/0/0/1/2/0000128037967f0d4b7ba748a80d5b248d1203f8.webp\n",
      "images/bedroom_val/0/0/0/0/1/5/000015122516efa29c25870b6b60fafe2fae1513.webp\n",
      "images/bedroom_val/0/0/0/0/1/c/00001c1755ac170e5382876232aec651f6bda841.webp\n",
      "images/bedroom_val/0/0/0/0/2/3/000023924aa8e512db983cba65e30cc106123ce3.webp\n",
      "images/bedroom_val/0/0/0/0/3/5/0000356acb787613fc8d8715cc6c182c05173535.webp\n",
      "images/bedroom_val/0/0/0/0/3/f/00003f8ec7ff5d59865ab2b6fb58bc663ace3b23.webp\n",
      "0. Processing images/bedroom_val/0/0/0/0/0/0/00000089629ce3ba87bae003073896ba01988dee.webp\n",
      "1. Processing images/bedroom_val/0/0/0/0/0/1/000001ec5684cb40f432f996f8a38e5d076114d8.webp\n",
      "2. Processing images/bedroom_val/0/0/0/0/0/3/0000036b25b1ae054cdf2e3ee954fe2d21db6ae0.webp\n",
      "3. Processing images/bedroom_val/0/0/0/0/0/b/00000b94a63ae2e1c08b4e7452d088156a9a8273.webp\n",
      "4. Processing images/bedroom_val/0/0/0/0/1/2/0000128037967f0d4b7ba748a80d5b248d1203f8.webp\n",
      "5. Processing images/bedroom_val/0/0/0/0/1/5/000015122516efa29c25870b6b60fafe2fae1513.webp\n",
      "6. Processing images/bedroom_val/0/0/0/0/1/c/00001c1755ac170e5382876232aec651f6bda841.webp\n",
      "7. Processing images/bedroom_val/0/0/0/0/2/3/000023924aa8e512db983cba65e30cc106123ce3.webp\n",
      "8. Processing images/bedroom_val/0/0/0/0/3/5/0000356acb787613fc8d8715cc6c182c05173535.webp\n",
      "9. Processing images/bedroom_val/0/0/0/0/3/f/00003f8ec7ff5d59865ab2b6fb58bc663ace3b23.webp\n"
     ]
    }
   ],
   "source": [
    "# playable version of DataGenerator()\n",
    "class FuckAround():\n",
    "    # list the bucket and directory within\n",
    "    bucketname = 'gs://lsun-roomsets'\n",
    "    directory = 'images/bedroom_val/'\n",
    "    \n",
    "    # loop through all the files and view first X images (count)\n",
    "    count = 0\n",
    "    max_count = 10\n",
    "    # store the raw img urls here\n",
    "    img_urls = []\n",
    "    for blob in bucket.list_blobs(prefix=directory):\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(blob.name)\n",
    "        count += 1\n",
    "        img_urls.append(blob.name)\n",
    "        \n",
    "    # store the resized images here\n",
    "    images = []\n",
    "    points = []\n",
    "    masks = []\n",
    "    \n",
    "    # CONSTANTS\n",
    "    # mask size limits\n",
    "    hole_min = 64\n",
    "    hole_max = 128\n",
    "    # batch limits\n",
    "    batch_size = 5\n",
    "    max_batches = 3\n",
    "    batch_count = 0\n",
    "    # image sizes\n",
    "    image_size = (256,256)\n",
    "    local_size = (128,128)\n",
    "    \n",
    "    for idx, img_url in enumerate(img_urls):\n",
    "        # we use tf...file_io.FileIO to grab the file\n",
    "        with file_io.FileIO(f'{bucketname}/{img_url}', 'rb') as f:\n",
    "            # and use PIL to convert into an RGB image\n",
    "            img = Image.open(f).convert('RGB')\n",
    "            # then convert the RGB image to an array so that cv2 can read it\n",
    "            img = np.asarray(img, dtype=\"uint8\")\n",
    "            # resize images\n",
    "            img_resized = cv2.resize(img, image_size)[:,:,::-1]\n",
    "            # take a look at the images\n",
    "            # cv2.imshow(f'image_{idx}_resized', img_resized)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyWindow(f'image_{idx}_resized')\n",
    "            # add the resized photo to self.images\n",
    "            images.append(img_resized)\n",
    "            print(f'{idx}. Processing {img_url}')\n",
    "            \n",
    "            # now lets create the random points where we will apply a mask (erase parts of image)\n",
    "            # recall that image_size=(256,256) and local_size=(128,128)\n",
    "            x1 = np.random.randint(0, image_size[0] - local_size[0] + 1)\n",
    "            y1 = np.random.randint(0, image_size[1] - local_size[1] + 1)\n",
    "            x2, y2 = np.array([x1, y1]) + np.array(local_size)\n",
    "            points.append([x1,y1,x2,y2])\n",
    "            \n",
    "            # and we also randomly generate width and height of those masks\n",
    "            w, h = np.random.randint(hole_min, hole_max, 2)\n",
    "            p1 = x1 + np.random.randint(0, local_size[0] - w)\n",
    "            q1 = y1 + np.random.randint(0, local_size[1] - h)\n",
    "            p2 = p1 + w\n",
    "            q2 = q1 + h\n",
    "            # now create the array of zeros\n",
    "            m = np.zeros((image_size[0], image_size[1], 1), dtype=np.uint8)\n",
    "            # everywhere there should be the mask, make the value one (everywhere else is zero)\n",
    "            m[q1:q2 + 1, p1:p2 + 1] = 1\n",
    "            # finally append it to the self.masks\n",
    "            masks.append(m)\n",
    "            \n",
    "            # print the batch of data when batch size reached\n",
    "            if len(images) == batch_size:\n",
    "#                 print(np.array(images).shape)\n",
    "#                 print(np.array(points).shape)\n",
    "#                 print(np.array(masks).shape)\n",
    "                inputs = np.asarray(images, dtype=np.float32) / 255\n",
    "                points = np.asarray(points, dtype=np.int32)\n",
    "                masks = np.asarray(masks, dtype=np.float32)\n",
    "                \n",
    "                # reset\n",
    "                images = []\n",
    "                points = []\n",
    "                masks = []\n",
    "                batch_count += 1\n",
    "                \n",
    "            if batch_count > max_batches:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerator\n",
    "Using a memory-efficient Python generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    # initialize by retreiving the photos\n",
    "    def __init__(self, bucketname, input_dir, image_size, local_size):\n",
    "        # bucketname = 'gs://lsun-roomsets'\n",
    "        # input_dir = 'images/bedroom_train/'\n",
    "        # image_size = (256,256)\n",
    "        # local_size = (128,128)\n",
    "        self.image_size = image_size\n",
    "        self.local_size = local_size\n",
    "        self.reset()\n",
    "        self.img_file_list = []\n",
    "        # for now we get max self.count photos and add them to self.img_file_list\n",
    "        for blob in bucket.list_blobs(prefix=input_dir):\n",
    "            self.img_file_list.append(blob.name)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_file_list)\n",
    "    \n",
    "    # we also track the preprocessed images, points, and masks\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "        self.points = []\n",
    "        self.masks = []\n",
    "    \n",
    "    # iterates over self.img_file_list and does preprocessing\n",
    "    def flow(self, batch_size, hole_min=64, hole_max=128):\n",
    "        np.random.shuffle(self.img_file_list)\n",
    "        for idx, img_url in enumerate(self.img_file_list):\n",
    "            # we use tf...file_io.FileIO to grab the file\n",
    "            with file_io.FileIO(f'{bucketname}/{img_url}', 'rb') as f:\n",
    "                # and use PIL to convert into an RGB image\n",
    "                img = Image.open(f).convert('RGB')\n",
    "                # then convert the RGB image to an array so that cv2 can read it\n",
    "                img = np.asarray(img, dtype=\"uint8\")\n",
    "                # resize images\n",
    "                img_resized = cv2.resize(img, self.image_size)[:,:,::-1]\n",
    "                # take a look at the images\n",
    "                # cv2.imshow(f'image_{idx}_resized', img_resized)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyWindow(f'image_{idx}_resized')\n",
    "                # add the resized photo to self.images\n",
    "                self.images.append(img_resized)\n",
    "\n",
    "                # now lets create the random location (aka. X,Y points) where we will apply a mask (aka. erase parts of image)\n",
    "                # recall that image_size=(256,256) and local_size=(128,128)\n",
    "                x1 = np.random.randint(0, self.image_size[0] - self.local_size[0] + 1)\n",
    "                y1 = np.random.randint(0, self.image_size[1] - self.local_size[1] + 1)\n",
    "                x2, y2 = np.array([x1, y1]) + np.array(self.local_size)\n",
    "                self.points.append([x1,y1,x2,y2])\n",
    "                # and we also randomly generate width and height of those masks\n",
    "                w, h = np.random.randint(hole_min, hole_max, 2)\n",
    "                p1 = x1 + np.random.randint(0, self.local_size[0] - w)\n",
    "                q1 = y1 + np.random.randint(0, self.local_size[1] - h)\n",
    "                p2 = p1 + w\n",
    "                q2 = q1 + h\n",
    "                # now create the array of zeros\n",
    "                m = np.zeros((self.image_size[0], self.image_size[1], 1), dtype=np.uint8)\n",
    "                # everywhere there should be the mask, make the value one (everywhere else is zero)\n",
    "                m[q1:q2 + 1, p1:p2 + 1] = 1\n",
    "                # finally append it to the self.masks\n",
    "                self.masks.append(m)\n",
    "\n",
    "                # yeild the batch of data when batch size reached\n",
    "                if len(self.images) == batch_size:\n",
    "                    images = np.asarray(self.images, dtype=np.float32) / 255\n",
    "                    points = np.asarray(self.points, dtype=np.int32)\n",
    "                    masks = np.asarray(self.masks, dtype=np.float32)\n",
    "                    self.reset()\n",
    "                    yield images, points, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Start the Training\n",
    "With hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import generic_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_shape = (256, 256, 3)\n",
    "local_shape = (128, 128, 3)\n",
    "batch_size = 96\n",
    "epochs = 500000\n",
    "g_epochs = int(epochs * 0.18) # should be 90k on generator\n",
    "d_epochs = int(epochs * 0.02) # should be 10k on discriminator\n",
    "alpha = 0.0004\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "# input/output directories\n",
    "bucketname = \"gs://lsun-roomsets\"\n",
    "output_dir = \"outputs/\"\n",
    "input_dir = \"images/bedroom_val/\"\n",
    "\n",
    "# data generator\n",
    "train_datagen = DataGenerator(bucketname, input_dir, input_shape[:2], local_shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2823s 9s/step - Disc Loss: : 7.9751 - Gen mse: : 0.0454\n",
      "300/300 [==============================] - 2598s 9s/step - Disc Loss: : 7.9712 - Gen mse: : 0.0265\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c99b10f80f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdreamt_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"outputs/images/epoch_{gen_img_count}_image.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"generator.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"discriminator.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# train over time\n",
    "for epoch in range(epochs):\n",
    "    # progress bar visualization (comment out in ML Engine)\n",
    "    progbar = generic_utils.Progbar(len(train_datagen))\n",
    "    for images, points, masks in train_datagen.flow(batch_size):\n",
    "        # and the matrix of ones that we depend on in the neural net to inverse masks\n",
    "        mask_inv = np.ones((len(images), input_shape[0], input_shape[1], 1))\n",
    "        # generate the inputs (images)\n",
    "        generated_img = gen_brain.predict([images, masks, mask_inv])\n",
    "        # generate the labels\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        # the gen and disc losses\n",
    "        g_loss = 0.0\n",
    "        d_loss = 0.0\n",
    "        \n",
    "        # we must train the neural nets seperately, and then together\n",
    "        # train generator for 90k epochs\n",
    "        if epoch < g_epochs:\n",
    "            # set the gen loss\n",
    "            g_loss = gen_brain.train_on_batch([images, points], valid)\n",
    "        # train discriminator alone for 90k epochs\n",
    "        # then train disc + gen for another 400k epochs. Total of 500k\n",
    "        else:\n",
    "            # throw in real unedited images with label VALID\n",
    "            d_loss_real = disc_brain.train_on_batch([images, points], valid)\n",
    "            # throw in A.I. generated images with label FAKE\n",
    "            d_loss_fake = disc_brain.train_on_batch([generated_img, points], fake)\n",
    "            # combine and set the disc loss\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            if epoch >= tc + td:\n",
    "                # train the entire brain\n",
    "                g_loss = brain.train_on_batch([images, masks, mask_inv, points], [images, valid])\n",
    "                # and update the generator loss\n",
    "                g_loss = g_loss[0] + alpha * g_loss[1]\n",
    "        # progress bar visualization (comment out in ML Engine)\n",
    "        progbar.add(images.shape[0], values=[(\"Disc Loss: \", d_loss), (\"Gen mse: \", g_loss)])\n",
    "        batch_count += 1\n",
    "        # save the generated image\n",
    "        last_img = generated_img[0]\n",
    "        last_img[:,:,0] = last_img[:,:,0]*255\n",
    "        last_img[:,:,1] = last_img[:,:,1]*255\n",
    "        last_img[:,:,2] = last_img[:,:,2]*255\n",
    "        dreamt_image = Image.fromarray(last_img.astype(int), 'RGB')\n",
    "        dreamt_image.save(f\"outputs/images/batch_{batch_count}_image.png\")\n",
    "        \n",
    "    gen_brain.save(f\"outputs/models/batch_{batch_count}_generator.h5\")\n",
    "    disc_brain.save(f\"outputs/models/batch_{batch_count}discriminator.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "Steps left to reach deployment on ML Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Healthy Model\n",
    "1. Verify that `last_img[:,:,0]*255` is actually how to convert 0-1 back to 0-255 RGB (ask a Python guy)\n",
    "2. Verify that our mis-organized gs://bucket/paths dont matter for wildcard `/**/*.png` image retrieval\n",
    "3. Verify that we can actually hold all the img_urls from gs://bucket/train/* in python memory (should be ok since ML 4ngine is serverless)\n",
    "5. Retrieve and display the accuracy/percision metrics during training\n",
    "6. Distinguish when to use model.train_on_batch() vs model.fit() vs model.fit_to_generator()\n",
    "7. Final audit that the model was made right\n",
    "8. Re-factor the iPython Notebook into a Python Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Building Tools\n",
    "1. Save the last generated image of each epoch to Google Cloud Storage\n",
    "2. Save the accuracy & percision metrics to gs://bucket during training\n",
    "2. Save last best checkpoint of model to gs://bucket. Also able to load model checkpoints to resume training (check if ML engine does that already - unlikely)\n",
    "5. Setup the arguements injection for ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Training\n",
    "1. Setup & Test single-GPU training on ML Engine\n",
    "2. Setup & Test multi-GPU training on ML Engine\n",
    "3. Train fully on ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
